{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: LSTNet Multi GPU and Distributed Training\n",
    "\n",
    "In this lab the LSTNet model which has been ported to use Amazon SageMaker is modified to be run using multipe GPUs and with distributed training.\n",
    "\n",
    "#### Pre-requisites\n",
    "It is assumed you are farmiliar with multi-GPU and distributed training using MXNet and SageMaker as covered in the previous labs ['Training on multiple GPUs with gluon'](../multiple_gpus_gluon/multiple_gpus_gluon.ipynb) and ['Distributed Training with Amazon SageMaker and gluon'](distributed_training_gluon/distributed_training_gluon.ipynb). Also it is assumed that you have just ported your LSTNet code to SageMaker in the ['Porting LSTNet to Amazon SageMaker'](porting_lstnet_to_sagemaker/porting_lstnet_to_sagemaker.ipynb) lab.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "There are three main steps required to scale the training using multiple GPUs and multiple hosts:\n",
    "\n",
    "1. Pass the appropriate kvstore parameter to the Gluon traininer. This specifies how parameters are synchronised between batches. In this case 'dist_device_sync' will be used which uses a parameter server to manage multiple hosts and performs the gradient updates on the GPUs when possible.\n",
    "\n",
    "2. Shard the training dataset. To perform distributed cluster training, the training dataset is split into shards with at least 1 shard per host. In this case it is split into 5 shards as 5 hosts will be used. Each host trains using only a portion of the dataset. The sharded training data is stored Amazon S3.\n",
    "\n",
    "3. Split each batch into portions and copy the portions onto one GPU per portion. In this case 4 GPUs will be used. Each GPU trains on only a portion of each batch. The gradients are summed over all GPUs at the end of the batch and all GPUs (and hosts when combining with distributed) are updated. These updates are performed on the GPU when possible. We need to perform the splitting, Gluon automatically manages the synchronising and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Variables\n",
    "\n",
    "Configure the following variables for your environment:\n",
    "\n",
    "1. bucket - The bucket name to be used to store the training data and model artifacts.\n",
    "2. prefix - The folder name which is used inside the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'eduthie-sagemaker-1'\n",
    "prefix = 'lstnet'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_dir = '../../data'\n",
    "data_file_path = os.path.join(data_dir,'electricity.txt')\n",
    "\n",
    "test_bucket_prefix = '/test/'\n",
    "single_host_train_bucket_prefix = '/train/single_host/'\n",
    "multiple_host_train_bucket_prefix = '/train/multiple_host/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "\n",
    "The first step is to load the electricity dataset from a file. The dataset itself is included in this github repo in the data directory. \n",
    "\n",
    "* The data is normalised so each reading is between 0 and 1. This is done by dividing each column by the maximum value of the column. A column is an electricity consumption time series for a single customer.\n",
    "\n",
    "There are 321 time series of electricity consumption with 26,304 time periods in each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26304, 321)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file_path,header=None)\n",
    "max_columns = df.max().astype(np.float64)\n",
    "df = df/max_columns # normalize\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Training and Test, Shard the Training Data, and Upload to S3\n",
    "\n",
    "The first 80% of the time series is used for training and the last 20% is used as a test set.\n",
    "\n",
    "The training set is sharded sequentially into 5 parts, one for each host in the cluster.\n",
    "\n",
    "These datasets are written to a csv file and then uploaded to Amazon S3 to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size 21043\n",
      "Test size 5261\n",
      "start 0\n",
      "end 4208\n",
      "start 4208\n",
      "end 8417\n",
      "start 8417\n",
      "end 12625\n",
      "start 12625\n",
      "end 16834\n",
      "start 16834\n",
      "end 21043\n",
      "Uploading file: ../../data/train_0.csv with 4208 rows\n",
      "Uploading to lstnet/train/multiple_host/train_0.csv\n",
      "Uploading file: ../../data/train_1.csv with 4209 rows\n",
      "Uploading to lstnet/train/multiple_host/train_1.csv\n",
      "Uploading file: ../../data/train_2.csv with 4208 rows\n",
      "Uploading to lstnet/train/multiple_host/train_2.csv\n",
      "Uploading file: ../../data/train_3.csv with 4209 rows\n",
      "Uploading to lstnet/train/multiple_host/train_3.csv\n",
      "Uploading file: ../../data/train_4.csv with 4209 rows\n",
      "Uploading to lstnet/train/multiple_host/train_4.csv\n"
     ]
    }
   ],
   "source": [
    "splits = 5\n",
    "train_frac = 0.8\n",
    "\n",
    "num_time_steps = len(df)\n",
    "split_index = int(num_time_steps*train_frac)\n",
    "train = df[0:split_index]\n",
    "print('Training size {}'.format(len(train)))\n",
    "test = df[split_index:]\n",
    "print('Test size {}'.format(len(test)))\n",
    "\n",
    "train_sets = []\n",
    "train_len = len(train)\n",
    "train_size = int(train_len)/splits\n",
    "for i in range(0,splits):\n",
    "    start = int(i*train_size)\n",
    "    end = int((i+1)*train_size)\n",
    "    print('start {}'.format(start))\n",
    "    print('end {}'.format(end))\n",
    "    if end < (train_len-1):\n",
    "        train_sets.append(train[start:end])\n",
    "    else:\n",
    "        train_sets.append(train[start:])\n",
    "\n",
    "\n",
    "test_file_path = os.path.join(data_dir,'test.csv')\n",
    "test.to_csv(test_file_path,header=None,index=False)\n",
    "train_file_path = os.path.join(data_dir,'train.csv')\n",
    "train.to_csv(train_file_path,header=None,index=False)\n",
    "\n",
    "client = boto3.client('s3')\n",
    "\n",
    "for i in range(0,splits):\n",
    "    file_path = os.path.join(data_dir,'train_{}.csv'.format(i))\n",
    "    print('Uploading file: {} with {} rows'.format(file_path,len(train_sets[i])))\n",
    "    train_sets[i].to_csv(file_path,header=None,index=False)\n",
    "    s3_path = prefix + '{}train_{}.csv'.format(multiple_host_train_bucket_prefix,i)\n",
    "    print('Uploading to {}'.format(s3_path))\n",
    "    client.upload_file(file_path, bucket, s3_path)\n",
    "\n",
    "client.upload_file(test_file_path, bucket, prefix + '{}test.csv'.format(test_bucket_prefix))\n",
    "client.upload_file(train_file_path, bucket, prefix + '{}train.csv'.format(single_host_train_bucket_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications to lstnet_sagemaker.py\n",
    "\n",
    "There are two main changes to the module:\n",
    "    \n",
    "1. Choose the kvstore as 'dist_device_sync' when multiple gpus and hosts are available.\n",
    "2. Split each batch into one part per GPU and copy each part to a separate GPU before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import mxnet as mx\r\n",
      "from mxnet import nd, gluon, autograd, kv\r\n",
      "import numpy as np\r\n",
      "from mxnet.gluon import nn, rnn\r\n",
      "import os\r\n",
      "from lstnet import LSTNet\r\n",
      "from timeseriesdataset import TimeSeriesData, TimeSeriesDataset\r\n",
      "import re\r\n",
      "import time\r\n",
      "\r\n",
      "def get_first_file_path_in_dir(input_dir):\r\n",
      "    \"\"\"\r\n",
      "    Returns the path of the first file found in the directory path provided,\r\n",
      "    or None if no file is found\r\n",
      "\r\n",
      "    :param input_dir: directory path to search\r\n",
      "    :return: file file\r\n",
      "    :rtype string\r\n",
      "    \"\"\"\r\n",
      "    for root, dirs, files in os.walk(input_dir):\r\n",
      "        for filename in files:\r\n",
      "            return os.path.join(input_dir,filename)\r\n",
      "    return None\r\n",
      "\r\n",
      "def get_file_path(input_dir,current_host,hosts):\r\n",
      "    file_path = None\r\n",
      "    if len(hosts) <= 1:\r\n",
      "        file_path = get_first_file_path_in_dir(input_dir)\r\n",
      "    else:\r\n",
      "        numbers_in_host_name = re.findall('\\d+', current_host)\r\n",
      "        index = int(numbers_in_host_name[0]) - 1\r\n",
      "        file_path = '{}/train_{}.csv'.format(input_dir,index)\r\n",
      "    return file_path\r\n",
      "\r\n",
      "def load_file(file_path,hyperparameters):\r\n",
      "    print('Loading file {}'.format(file_path))\r\n",
      "    if not file_path:\r\n",
      "        print('Could not load data file: ' + file_path)\r\n",
      "        return None\r\n",
      "    ts_data = TimeSeriesData(file_path,\r\n",
      "        window=hyperparameters['window'],\r\n",
      "        horizon=hyperparameters['horizon'],\r\n",
      "        train_ratio=1.0)\r\n",
      "    return ts_data\r\n",
      "\r\n",
      "def train(\r\n",
      "    hyperparameters,\r\n",
      "    input_data_config,\r\n",
      "    channel_input_dirs,\r\n",
      "    output_data_dir,\r\n",
      "    model_dir,\r\n",
      "    num_gpus,\r\n",
      "    num_cpus,\r\n",
      "    hosts,\r\n",
      "    current_host,\r\n",
      "    **kwargs):\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "    [Required]\r\n",
      "\r\n",
      "    Runs Apache MXNet training. Amazon SageMaker calls this function with information\r\n",
      "    about the training environment. When called, if this function returns an\r\n",
      "    object, that object is passed to a save function.  The save function\r\n",
      "    can be used to serialize the model to the Amazon SageMaker training job model\r\n",
      "    directory.\r\n",
      "\r\n",
      "    The **kwargs parameter can be used to absorb any Amazon SageMaker parameters that\r\n",
      "    your training job doesn't need to use. For example, if your training job\r\n",
      "    doesn't need to know anything about the training environment, your function\r\n",
      "    signature can be as simple as train(**kwargs).\r\n",
      "\r\n",
      "    Amazon SageMaker invokes your train function with the following python kwargs:\r\n",
      "\r\n",
      "    Args:\r\n",
      "        - hyperparameters: The Amazon SageMaker Hyperparameters dictionary. A dict\r\n",
      "            of string to string.\r\n",
      "        - input_data_config: The Amazon SageMaker input channel configuration for\r\n",
      "            this job.\r\n",
      "        - channel_input_dirs: A dict of string-to-string maps from the\r\n",
      "            Amazon SageMaker algorithm input channel name to the directory containing\r\n",
      "            files for that input channel. Note, if the Amazon SageMaker training job\r\n",
      "            is run in PIPE mode, this dictionary will be empty.\r\n",
      "        - output_data_dir:\r\n",
      "            The Amazon SageMaker output data directory. After the function returns, data written to this\r\n",
      "            directory is made available in the Amazon SageMaker training job\r\n",
      "            output location.\r\n",
      "        - model_dir: The Amazon SageMaker model directory. After the function returns, data written to this\r\n",
      "            directory is made available to the Amazon SageMaker training job\r\n",
      "            model location.\r\n",
      "        - num_gpus: The number of GPU devices available on the host this script\r\n",
      "            is being executed on.\r\n",
      "        - num_cpus: The number of CPU devices available on the host this script\r\n",
      "            is being executed on.\r\n",
      "        - hosts: A list of hostnames in the Amazon SageMaker training job cluster.\r\n",
      "        - current_host: This host's name. It will exist in the hosts list.\r\n",
      "        - kwargs: Other keyword args.\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        - (object): Optional. An Apache MXNet model to be passed to the model\r\n",
      "            save function. If you do not return anything (or return None),\r\n",
      "            the save function is not called.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    train_file_path = get_file_path(channel_input_dirs['train'],current_host,hosts)\r\n",
      "    print('Train file path {}'.format(train_file_path))\r\n",
      "    test_file_path = get_first_file_path_in_dir(channel_input_dirs['test'])\r\n",
      "    print('Test file path {}'.format(test_file_path))\r\n",
      "    ts_data_train = load_file(train_file_path,hyperparameters)\r\n",
      "    ts_data_test = load_file(test_file_path,hyperparameters)\r\n",
      "\r\n",
      "    ctx = [mx.cpu(i) for i in range(num_cpus)]\r\n",
      "    if num_gpus > 0:\r\n",
      "        ctx = ctx = [mx.gpu(i) for i in range(num_gpus)]\r\n",
      "    print('Running on {}'.format(ctx))\r\n",
      "    print('Hosts {}'.format(hosts))\r\n",
      "    print('Current Host {}'.format(current_host))\r\n",
      "\r\n",
      "    net = LSTNet(\r\n",
      "        num_series=ts_data_train.num_series,\r\n",
      "        conv_hid=hyperparameters['conv_hid'],\r\n",
      "        gru_hid=hyperparameters['gru_hid'],\r\n",
      "        skip_gru_hid=hyperparameters['skip_gru_hid'],\r\n",
      "        skip=hyperparameters['skip'],\r\n",
      "        ar_window=hyperparameters['ar_window'])\r\n",
      "\r\n",
      "    net.initialize(init=mx.init.Xavier(factor_type=\"in\", magnitude=2.34), ctx=ctx)\r\n",
      "\r\n",
      "    kvstore = 'local'\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "    print('kvstore {}'.format(kvstore))\r\n",
      "    store = kv.create(kvstore)\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "        kvstore=store,\r\n",
      "        optimizer='adam',\r\n",
      "        optimizer_params={'learning_rate': hyperparameters['learning_rate'], 'clip_gradient': hyperparameters['clip_gradient']})\r\n",
      "\r\n",
      "    batch_size = hyperparameters['batch_size']\r\n",
      "    train_data_loader = gluon.data.DataLoader(\r\n",
      "        ts_data_train.train, batch_size=batch_size, shuffle=True, num_workers=16, last_batch='discard')\r\n",
      "    test_data_loader = gluon.data.DataLoader(\r\n",
      "        ts_data_test.train, batch_size=batch_size, shuffle=True, num_workers=16, last_batch='discard')\r\n",
      "\r\n",
      "    epochs = hyperparameters['epochs']\r\n",
      "    print(\"Training Start\")\r\n",
      "    metric = mx.metric.RMSE()\r\n",
      "    tic = time.time()\r\n",
      "    for e in range(epochs):\r\n",
      "        metric.reset()\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        for data, label in train_data_loader:\r\n",
      "            batch_forward_backward(data,label,ctx,net,trainer,batch_size,metric)\r\n",
      "        name, value = metric.get()\r\n",
      "        print(\"Epoch {}: {} {} time {:.4f} s\".format(e, name, value, time.time()-epoch_start_time))\r\n",
      "\r\n",
      "    # Calculate the test RMSE when training has finished\r\n",
      "    validate(train_data_loader,metric,ctx,net)\r\n",
      "\r\n",
      "    print(\"Total training time: {}\".format(time.time()-tic))\r\n",
      "\r\n",
      "    if not os.path.exists(output_data_dir):\r\n",
      "        os.makedirs(output_data_dir)\r\n",
      "    net.save_params(os.path.join(output_data_dir,'lstnet_params.params'))\r\n",
      "    print(\"Training End\")\r\n",
      "    return\r\n",
      "\r\n",
      "def validate(data_loader,metric,ctx,net):\r\n",
      "    metric.reset()\r\n",
      "    for data, label in data_loader:\r\n",
      "        data_split = gluon.utils.split_and_load(data, ctx)\r\n",
      "        label_split = gluon.utils.split_and_load(label, ctx)\r\n",
      "        for X,Y in zip(data_split,label_split):\r\n",
      "            metric.update(Y,net(X))\r\n",
      "    name, value = metric.get()\r\n",
      "    print('Final {} {}'.format(name,value))\r\n",
      "    return name,value\r\n",
      "\r\n",
      "def batch_forward_backward(data, label, ctx, net, trainer, batch_size, metric):\r\n",
      "    l1 = gluon.loss.L1Loss()\r\n",
      "    data = gluon.utils.split_and_load(data, ctx)\r\n",
      "    label = gluon.utils.split_and_load(label, ctx)\r\n",
      "    losses = []\r\n",
      "    outputs = []\r\n",
      "    with autograd.record():\r\n",
      "        for X, Y in zip(data,label):\r\n",
      "            z = net(X)\r\n",
      "            loss = l1(z,Y)\r\n",
      "            losses.append(loss)\r\n",
      "            outputs.append(z)\r\n",
      "    autograd.backward(losses)\r\n",
      "    trainer.step(batch_size)\r\n",
      "    metric.update(label,outputs)\r\n"
     ]
    }
   ],
   "source": [
    "!cat lstnet_sagemaker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Locally\n",
    "\n",
    "To make sure there are no obvious bugs in the code, the train() function is called in the notebook. This is done with 1 epoch to verify that it executed correctly. There are also some basic unit tests included in the directory. A notebook instance with a GPU is requred to execute the following steps. Otherwise skip on to deploying with SageMaker.\n",
    "\n",
    "The key parameters to the train() function in this case are:\n",
    "\n",
    "        - hyperparameters: The Amazon SageMaker Hyperparameters dictionary. A dict\n",
    "            of string to string.\n",
    "        - channel_input_dirs: A dict of string-to-string maps from the\n",
    "            Amazon SageMaker algorithm input channel name to the directory containing\n",
    "            files for that input channel. Note, if the Amazon SageMaker training job\n",
    "            is run in PIPE mode, this dictionary will be empty.\n",
    "        - output_data_dir:\n",
    "            The Amazon SageMaker output data directory. After the function returns, data written to this\n",
    "            directory is made available in the Amazon SageMaker training job\n",
    "            output location.\n",
    "        - num_gpus: The number of GPU devices available on the host this script\n",
    "            is being executed on.\n",
    "        - num_cpus: The number of CPU devices available on the host this script\n",
    "            is being executed on.\n",
    "        - hosts: A list of hostnames in the Amazon SageMaker training job cluster.\n",
    "        - current_host: This host's name. It will exist in the hosts list.\n",
    "        - kwargs: Other keyword args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ../../data/train_1.csv\n",
      "Test file path ../../data/train_1.csv\n",
      "Loading file ../../data/train_1.csv\n",
      "Loading file ../../data/train_1.csv\n",
      "Is it a file True\n",
      "Data length 4209\n",
      "Loading file ../../data/train_1.csv\n",
      "Loading file ../../data/train_1.csv\n",
      "Is it a file True\n",
      "Data length 4209\n",
      "Running on [gpu(0)]\n",
      "Hosts ['alg-1']\n",
      "Current Host alg-1\n",
      "kvstore device\n",
      "Training Start\n",
      "Epoch 0: rmse 0.3383692430873071\n",
      "Final rmse 0.4321038925103963\n",
      "Total training time: 64.28396916389465\n",
      "Training End\n"
     ]
    }
   ],
   "source": [
    "from lstnet_sagemaker import train\n",
    "hyperparameters = {\n",
    "    'conv_hid' : 10,\n",
    "    'gru_hid' : 10,\n",
    "    'skip_gru_hid' : 2,\n",
    "    'skip' : 5,\n",
    "    'ar_window' : 6,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.01,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 128,\n",
    "    'epochs' : 1\n",
    "}\n",
    "channel_input_dirs = {'train':data_dir,'test':data_dir}\n",
    "train(\n",
    "    hyperparameters = hyperparameters,\n",
    "    input_data_config = None,\n",
    "    channel_input_dirs = channel_input_dirs,\n",
    "    output_data_dir = os.path.join(data_dir,'output'),\n",
    "    model_dir = None,\n",
    "    num_gpus = 1,\n",
    "    num_cpus = 1,\n",
    "    hosts = ['alg-1'],\n",
    "    current_host = 'alg-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Hyperparameters\n",
    "\n",
    "One parameter to watch when switching to multi-GPU is the batch-size. As each batch is split across 4 GPUs it is common to increase the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_hid' : 100,\n",
    "    'gru_hid' : 100,\n",
    "    'skip_gru_hid' : 5,\n",
    "    'skip' : 24,\n",
    "    'ar_window' : 24,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 512,\n",
    "    'epochs' : 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigger the training job using the SageMaker Python API.\n",
    "\n",
    "The final step is to trigger the training job using the high-level Python API. A lower-level API is also available for more detailed control of the parameters.\n",
    "\n",
    "The main differences to not from the single host and GPU case is that the train_instance_count is set to 5 to run on a 5 node cluster and the train_instance_type of ml.p3.8xlarge is chosen with 4 GPUs. SageMaker automatically bootstraps an MXNet cluster with 5 nodes.\n",
    "\n",
    "First an estimator is created with sagemaker.mxnet.MXNet. The inputs are:\n",
    "\n",
    "* entry_point='lstnet_sagemaker.py' - The module used to run the training by calling the train() function\n",
    "* source_dir='.' - An optional directory containing code with is copied onto the SageMaker training hosts and made available to the training script.\n",
    "* role=role - The IAM role which is given to the training hosts giving them privileges such as access to the S3 bucket.\n",
    "* output_path='s3://{}/{}/output'.format(bucket, prefix) - The S3 bucket to store artifacts such as the model parameters.\n",
    "* train_instance_count=5 - The number of hosts used for training. Using a number > 1 will start a cluster. To take advantage of this the training data is sharded.\n",
    "* train_instance_type='ml.p3.8xlarge' - The EC2 instance type to be used for training hosts. In this case the latest generation p3 is chosen with 4 Nvidia Tesla v100 GPUs.\n",
    "* hyperparameters=hyperparameters - The hyperparameter dictionary made available to the train() function in the endpoint script.\n",
    "\n",
    "Then the fit() method of the estimator is called. The parameters are:\n",
    "\n",
    "* inputs - A dictionary containing the URLs in S3 of the 'train' data directory and the 'test' data directory.\n",
    "* wait - This is specified as False so the fit() method returns immediately after the training job is created. Go to the SageMaker console to monitor the progress of the job. Set wait to True to block and see the progress of the training job output in the notebook.\n",
    "\n",
    "2 different versions are run to compare traning speeds:\n",
    "\n",
    "1. 5 hosts with 4 GPUs\n",
    "2. 5 hosts with 1 GPU\n",
    "\n",
    "Experiment with more combinations to improve performance. Can you find the most efficient batch-size vs harware combination for this network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-07-19-12-00-25-501\n"
     ]
    }
   ],
   "source": [
    "lstnet1 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=5,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    hyperparameters=hyperparameters)\n",
    "lstnet1.fit(inputs={'train': 's3://{}/{}{}'.format(bucket, prefix, multiple_host_train_bucket_prefix),\n",
    "    'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-07-19-12-03-13-796\n"
     ]
    }
   ],
   "source": [
    "lstnet3 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters=hyperparameters)\n",
    "lstnet3.fit(inputs={'train': 's3://{}/{}{}'.format(bucket, prefix, single_host_train_bucket_prefix),\n",
    "    'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
