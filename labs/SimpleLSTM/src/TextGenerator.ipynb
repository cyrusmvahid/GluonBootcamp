{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CharLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "from gluonnlp.data.utils import slice_sequence\n",
    "from mxnet.gluon.data import SimpleDataset\n",
    "from gluonnlp.data.utils import _get_home_dir\n",
    "from gluonnlp.data.registry import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When passing keyword arguments to `register`, they are checked to be valid keyword arguments for the registered \n",
    "Dataset class constructor and are saved in the registry.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "@register(segment=['train', 'test'])\n",
    "class TimeMachineDataLoader(SimpleDataset):\n",
    "    def __init__(self, segment='train', \n",
    "                 root=os.path.join(_get_home_dir(), 'data', 'word_generator')):\n",
    "        self._data_file = {'train': ('train.txt', ''),\n",
    "                          'test': ('test.txt', '')}\n",
    "        root = os.path.expanduser(root)\n",
    "        if not os.path.isdir(root):\n",
    "            os.makedirs(root)\n",
    "        self._root = root\n",
    "        self._segment = segment\n",
    "        self._get_data()\n",
    "        self._file_path = self._get_file_path()\n",
    "        \n",
    "        super(TimeMachineDataLoader, self).__init__(self._read_data())\n",
    "        @property\n",
    "        def file_path(self):\n",
    "            return self._file_path        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "    def _get_data(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path) or not check_sha1(path, data_hash):\n",
    "            download('http://archive.org/stream/thetimemachine00035gut/35.txt', path=root)\n",
    "        with open(os.path.join(root, '35.txt')) as f:\n",
    "            raw_data = f.read()\n",
    "        raw_data = raw_data[44332: -24182]\n",
    "        raw_data_val = raw_data[-len(raw_data)//3:]\n",
    "        raw_data = raw_data[:2*len(raw_data)//3]\n",
    "        with open(os.path.join(root, 'train.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data)\n",
    "        \n",
    "\n",
    "        with open(os.path.join(root, 'test.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data_val)\n",
    "                \n",
    "\n",
    "    def _read_data(self):\n",
    "        with open(os.path.join(self._root, self._segment+'.txt')) as f:\n",
    "            samples = list(f.read())\n",
    "        return samples\n",
    "    \n",
    "    def _get_file_path(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError\n",
    "        return path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/data/registry.py:91: UserWarning: \u001b[91mNew dataset __main__.TimeMachineDataLoader registered with name timemachinedataloader isoverriding existing dataset __main__.TimeMachineDataLoader\u001b[0m\n",
      "  return register_(class_)\n"
     ]
    }
   ],
   "source": [
    "@register(segment=['train', 'test'])\n",
    "class TimeMachineDataLoader(SimpleDataset):\n",
    "    def __init__(self, segment='train', \n",
    "                 root=os.path.join(_get_home_dir(), 'data', 'word_generator')):\n",
    "        self._data_file = {'train': ('train.txt', ''),\n",
    "                          'test': ('test.txt', '')}\n",
    "        root = os.path.expanduser(root)\n",
    "        if not os.path.isdir(root):\n",
    "            os.makedirs(root)\n",
    "        self._root = root\n",
    "        self._segment = segment\n",
    "        self._get_data()\n",
    "        self._file_path = self._get_file_path()\n",
    "        \n",
    "        super(TimeMachineDataLoader, self).__init__(self._read_data())\n",
    "        \n",
    "    @property\n",
    "    def file_path(self):\n",
    "        return self._file_path\n",
    "    \n",
    "    def _get_data(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path) or not check_sha1(path, data_hash):\n",
    "            download('http://archive.org/stream/thetimemachine00035gut/35.txt', path=root)\n",
    "        with open(os.path.join(root, '35.txt')) as f:\n",
    "            raw_data = f.read()\n",
    "        raw_data = raw_data[44332: -24182]\n",
    "        raw_data_val = raw_data[-len(raw_data)//3:]\n",
    "        raw_data = raw_data[:2*len(raw_data)//3]\n",
    "        with open(os.path.join(root, 'train.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data)\n",
    "        \n",
    "\n",
    "        with open(os.path.join(root, 'test.txt'), 'w+') as output_file:\n",
    "            output_file.write(raw_data_val)\n",
    "                \n",
    "\n",
    "    def _read_data(self):\n",
    "        with open(os.path.join(self._root, self._segment+'.txt')) as f:\n",
    "            samples = list(f.read())\n",
    "        return samples\n",
    "    \n",
    "    def _get_file_path(self):\n",
    "        data_file_name, data_hash = self._data_file[self._segment]\n",
    "        root = self._root\n",
    "        path = os.path.join(root, data_file_name)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError\n",
    "        return path\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataSet and Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class CharLevelDataSet(SimpleDataset):\n",
    "    def __init__(self, dataset, tokenizer=nlp.data.SpacyTokenizer('en')):\n",
    "        self._tokenizer = tokenizer\n",
    "        #self._dataset = self._tokenizer(dataset[:])\n",
    "        self._dataset = dataset\n",
    "    \n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self._dataset\n",
    "    \n",
    "    def batchify(self, vocab, batch_size):\n",
    "        data = self._dataset[:]\n",
    "        sample_len = len(data) // batch_size\n",
    "        return mx.nd.array(vocab[data[:sample_len * batch_size]]).reshape((batch_size, -1)).T\n",
    "    \n",
    "    def bptt_batchify(self, bptt, vocab, batch_size):\n",
    "        data = self.batchify(vocab, batch_size)\n",
    "        batches = slice_sequence(data, bptt+1, overlap=1)\n",
    "        return SimpleDataset(batches).transform(lambda x: (x[:min(len(x)-1, bptt), :], x[1:, :]))\n",
    "        \n",
    "\n",
    "nlp.data.\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Alternative method\n",
    "The above code is created for training purpose. normally you would inherit a single dataset class from `gluonnlp.data.LanguageModelDataset` that provides `batchify` and `bptt-batchify` through inheritence.\n",
    "For more information please check:\n",
    "1. `_WikiText`, and `WikiText2` classes in [github](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/data/language_model.py)\n",
    "2. `Sentiment` class in [github](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/data/sentiment.py) or [gluonnlp API Docs](https://gluon-nlp.mxnet.io/api/data.html#transforms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Transformation\n",
    "In order to create character level LSTM, you would beed to identify ```python tokenizer=lambda s : s```. For word level tokenizer leave the tokenizer to default. You can also use one of the supported transformers you can import from ```gluonnlp.data```:\n",
    "- NLTKMosesTokenizer\n",
    "- SpacyTokenizer \n",
    "- JiebaTokenizer\n",
    "- NLTKStanfordSegmenter\n",
    "for more information and source code please check [github](https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/data/transforms.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on ```gluonnlp.data.utils.slice_sequence()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 3, overlap = 1: [[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]]\n",
      "length = 3, overlap = 2: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "a = [0,1,2,3,4,5,6,7,8]\n",
    "print(\"length = 3, overlap = 1: {}\".format(slice_sequence(a, length=3, overlap=1)))\n",
    "print(\"length = 3, overlap = 2: {}\".format(slice_sequence(a, length=3, overlap=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape= (5, 5)\n",
      "length = 3, overlap = 1: [\n",
      "[[-1.88121688 -0.48236853 -0.0410547   0.32565448 -1.16651702]\n",
      " [ 0.20766489 -1.60068631 -0.33250362 -0.36500821 -0.4170818 ]\n",
      " [ 0.67592829 -0.48124811 -1.44484794 -0.40208578 -1.3330071 ]]\n",
      "<NDArray 3x5 @cpu(0)>, \n",
      "[[ 0.67592829 -0.48124811 -1.44484794 -0.40208578 -1.3330071 ]\n",
      " [ 0.24919313 -0.35250196 -0.46159557 -1.61733854  0.65237534]\n",
      " [-2.08035088 -0.8373692  -0.58245718 -0.73683733  0.57604754]]\n",
      "<NDArray 3x5 @cpu(0)>]\n"
     ]
    }
   ],
   "source": [
    "a = nd.random_normal(shape=(5,5))\n",
    "print(\"a.shape= {}\".format(a.shape))\n",
    "print(\"length = 3, overlap = 1: {}\".format(slice_sequence(a, length=3, overlap=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters and EnvironmentVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.idx_to_token)\n",
    "num_embd = 256\n",
    "num_hidden = 512\n",
    "num_layers = 3\n",
    "opt = 'sgd'\n",
    "lr = .001\n",
    "momentum = .9\n",
    "wd = 0\n",
    "num_gpus = min(16, mx.context.num_gpus())\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "batch_size = 64\n",
    "grad_clip = 0.25\n",
    "log_interval = 200\n",
    "model_name=\"CharLSTM\"\n",
    "dataset_name=\"TimeMachine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python mx.context.num_gpus()``` is a newly merged method to context and does not exist in mxnet 1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Pipeline\n",
    "1. Downnload and split the data file.\n",
    "2. Instantiating datasets objects from ```CharLevelDataset``` in order to be able to create batches.\n",
    "3. Creating vocabulary by instantiating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Downloading and splitting data files\n",
    "train_dataset, test_dataset = [TimeMachineDataLoader(segment=segment, root='../data/text_generator')\n",
    "                               for segment in ['train', 'test']]\n",
    "\n",
    "# Crearing CharLevelDataSet that supports batchifying\n",
    "train_data = CharLevelDataSet(train_dataset)\n",
    "test_data = CharLevelDataSet(test_dataset)\n",
    "\n",
    "#Creating vocabulary\n",
    "vocab = nlp.vocab.Vocab(nlp.data.Counter(train_dataset[:] + test_dataset[:]), \n",
    "                        padding_token=None, \n",
    "                        eos_token=None, \n",
    "                        bos_token=None)\n",
    "#Creating iterable training and test data\n",
    "train_data, test_data = [x.bptt_batchify(bptt=129, vocab=vocab, batch_size=batch_size)\n",
    "                        for x in [train_data, test_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: (\n",
      "[[ 11.   5.   2. ...,   1.   1.   2.]\n",
      " [ 17.  19.  22. ...,  25.  11.  11.]\n",
      " [ 42.   1.  28. ...,   1.   2.  22.]\n",
      " ..., \n",
      " [ 23.   4.   5. ...,   1.  10.   8.]\n",
      " [  1.  12.  12. ...,  24.   2.   1.]\n",
      " [ 49.   6.  20. ...,   2.   2.  26.]]\n",
      "<NDArray 129x64 @cpu(0)>, \n",
      "[[ 17.  19.  22. ...,  25.  11.  11.]\n",
      " [ 42.   1.  28. ...,   1.   2.  22.]\n",
      " [  7.   4.   1. ...,  15.   2.   1.]\n",
      " ..., \n",
      " [  1.  12.  12. ...,  24.   2.   1.]\n",
      " [ 49.   6.  20. ...,   2.   2.  26.]\n",
      " [  6.   5.   1. ...,  19.   8.   2.]]\n",
      "<NDArray 129x64 @cpu(0)>)\n",
      "\n",
      "testdata: (\n",
      "[[ 13.   5.   9. ...,   5.   1.   2.]\n",
      " [ 20.   3.   2. ...,   1.  11.   1.]\n",
      " [  1.   1.   1. ...,  19.   7.   4.]\n",
      " ..., \n",
      " [ 12.  14.   2. ...,  12.   5.   3.]\n",
      " [  4.  19.  17. ...,   7.  11.   3.]\n",
      " [  3.   9.  16. ...,   5.   1.   2.]]\n",
      "<NDArray 129x64 @cpu(0)>, \n",
      "[[ 20.   3.   2. ...,   1.  11.   1.]\n",
      " [  1.   1.   1. ...,  19.   7.   4.]\n",
      " [ 13.   7.   7. ...,  12.   8.   5.]\n",
      " ..., \n",
      " [  4.  19.  17. ...,   7.  11.   3.]\n",
      " [  3.   9.  16. ...,   5.   1.   2.]\n",
      " [  7.   7.   4. ...,  19.   3.  10.]]\n",
      "<NDArray 129x64 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "print(\"traindata: {}\\n\\ntestdata: {}\".format(train_data[0], test_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(gluon.Block):\n",
    "    def __init__(self, vocab_size, num_embd, num_hidden, num_layers, dropout=.5, **kwargs):\n",
    "        super(LSTMModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = gluon.nn.Dropout(dropout)\n",
    "            self.encoder = gluon.nn.Embedding(vocab_size, num_embd, weight_initializer=mx.init.Uniform(.1))\n",
    "            self.lstm = gluon.rnn.LSTM(hidden_size=num_hidden, \n",
    "                                       num_layers=num_layers, \n",
    "                                       dropout=dropout, \n",
    "                                       input_size = num_embd)\n",
    "            self.decoder = gluon.nn.Dense(units=vocab_size, in_units=num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        #print(\"EMB_SHAPE: {}\".format(emb.shape))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        #print(\"OUTPUT_SHAPE_IN_MODEL: {}\".format(output.shape))\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.lstm.begin_state(*args, **kwargs)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Support in ```gluon.rnn``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "||||\n",
    "|:-|:-|:-|\n",
    "|Recurrent Layers|Recurruent Layers can be used in the ```Sequential``` block with other nn types of layers|\n",
    "||RNN|Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence. in order to make a layer **bi-directional** you would need to change the default behavious by passing ```python bidirectional=True``` to the constructor|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.RNN)|\n",
    "||LSTM|Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.In order to make a layer **bi-directional** you would need to change the default behavious by passing ```python bidirectional=True``` to the constructor|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.LSTM)|\n",
    "||GRU|Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRU)|\n",
    "|Recurrent Cells|In order to gain fine-grained control over implementation of your network you can use cells|\n",
    "||RNNCell|Elman RNN recurrent neural network cell.|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.RNNCell)|\n",
    "||LSTM Cell|Long-Short Term Memory (LSTM) network cell.|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.LSTMCell)|\n",
    "||GRU Cell|Gated Rectified Unit (GRU) network cell.|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRUCell)|\n",
    "||SequentialRNNCell|Sequentially stacking multiple RNN cells.|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.SequentialRNNCell)|\n",
    "||DropoutCell|Applied dropout on a cell|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.DropoutCell)|\n",
    "||ResidualCell|Adds residual connections|[docs](https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.ResidualCell)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstmmodel9_ (\n",
       "  Parameter lstmmodel9_embedding0_weight (shape=(68, 256), dtype=float32)\n",
       "  Parameter lstmmodel9_lstm0_l0_i2h_weight (shape=(2048, 256), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l0_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l0_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l0_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l1_i2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l1_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l1_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l1_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l2_i2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l2_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l2_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_lstm0_l2_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter lstmmodel9_dense0_weight (shape=(68, 512), dtype=float32)\n",
       "  Parameter lstmmodel9_dense0_bias (shape=(68,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size=len(vocab.idx_to_token), num_embd=256, num_hidden=512, num_layers=3)\n",
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr, 'momentum': momentum, 'wd': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detaching Gradients\n",
    "We need to detach gradient for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        #print(\"DATA_SHAPE: {}\".format(data.shape))\n",
    "        target = target.reshape((-1, )).as_in_context(ctx)\n",
    "        #print(\"TARGET_SHAPE: {}\".format(target.shape))\n",
    "        output, hidden = model(data, hidden)\n",
    "        #print(\"OUTPUT_SHAPE: {}\".format(output.shape))\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output,target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.21955379853613"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, data_source=test_data, batch_size=batch_size,ctx=ctx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "def train(model, train_data, val_data, test_data, epochs, lr, context):\n",
    "...\n",
    "    for epoch in range(epochs):\n",
    "    ...\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output, y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L / X.size)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "            trainer.step(1)\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "...    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr, context):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output, y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L / X.size)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L), \n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('../model/{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 842.32 samples/s\n",
      "[Epoch 0] time cost 1.23s, valid loss 3.07, valid ppl 21.58\n",
      "test loss 3.07, test ppl 21.58\n",
      "Total training throughput 616.87 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model=model, train_data=train_data, val_data=test_data, test_data=test_data, epochs=1, lr=lr, context=ctx)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
